{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b817de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py #\n",
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession.builder.appName(\"FlowerPreprocessing\").getOrCreate()\n",
    "\n",
    "# path to dataset\n",
    "# (each folder = one flower class)\n",
    "base_dir = \"/home/sat3812/projects/smallProject4/flowers\"\n",
    "\n",
    "# list the class names (folder names)\n",
    "classes = os.listdir(base_dir)\n",
    "\n",
    "# create a Spark dataframe manually from file paths and labels\n",
    "data = []\n",
    "\n",
    "for label, class_name in enumerate(classes):\n",
    "    class_folder = os.path.join(base_dir, class_name)\n",
    "\n",
    "    # Loop through all image files in each class\n",
    "    for img_file in os.listdir(class_folder):\n",
    "        full_path = os.path.join(class_folder, img_file)\n",
    "        data.append((full_path, class_name, label))\n",
    "\n",
    "# create Spark DataFrame\n",
    "df = spark.createDataFrame(data, [\"path\", \"label_str\", \"label_int\"])\n",
    "\n",
    "# shuffle data randomly\n",
    "df = df.orderBy(F.rand(seed = 42))\n",
    "\n",
    "# 80/20 split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed = 42)\n",
    "\n",
    "# save to CSV for the training script\n",
    "train_df.write.mode(\"overwrite\").csv(\"train_data.csv\", header = True)\n",
    "test_df.write.mode(\"overwrite\").csv(\"test_data.csv\", header = True)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(\"Training samples:\", train_df.count())\n",
    "print(\"Test samples:\", test_df.count())\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725011cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cnn.py #\n",
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# dataset class to load flower images\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform = None):\n",
    "        # store the dataframe and transforms\n",
    "        self.df = df.reset_index(drop = True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of samples\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get row\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # load image from file path\n",
    "        img_path = row[\"path\"]\n",
    "        label = int(row[\"label_int\"])\n",
    "\n",
    "        # open image in RGB mode\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # apply transforms (resize, tensor, normalize)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 128 -> 64\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 64 -> 32\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 32 -> 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 16 -> 8\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        # fully connected classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256), # changed from 32 * 32 * 32, 128, then 64 * 16 * 16, 256, then 128 * 8 * 8, 256\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # dropout to reduce overfitting, 0.5 -> 0.4\n",
    "            nn.Linear(256, num_classes) # changed from 128\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# training pipeline\n",
    "def train_flower_cnn():\n",
    "\n",
    "    # create spark session\n",
    "    spark = (SparkSession.builder.appName(\"FlowerCNNTraining\").getOrCreate())\n",
    "    # suppress spark backend logs\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    # spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # paths to csv files\n",
    "    train_csv_path = \"/home/sat3812/projects/smallProject4/train.csv\"\n",
    "    test_csv_path = \"/home/sat3812/projects/smallProject4/test.csv\"\n",
    "\n",
    "    # read csvs with spark\n",
    "    train_df_spark = spark.read.csv(train_csv_path, header = True, inferSchema = True)\n",
    "    test_df_spark = spark.read.csv(test_csv_path, header = True, inferSchema = True)\n",
    "\n",
    "    # show a preview of data\n",
    "    print(\"Train DataFrame preview:\")\n",
    "    train_df_spark.show(5)\n",
    "    print(\"Test DataFrame preview:\")\n",
    "    test_df_spark.show(5)\n",
    "\n",
    "    print(\"Train rows:\", train_df_spark.count())\n",
    "    print(\"Test rows:\", test_df_spark.count())\n",
    "\n",
    "    # convert spark DataFrames to pandas\n",
    "    train_df = train_df_spark.toPandas()\n",
    "    test_df = test_df_spark.toPandas()\n",
    "\n",
    "    # stop spark\n",
    "    spark.stop()\n",
    "\n",
    "    # image transforms with augmentation\n",
    "    img_transforms = transforms.Compose([\n",
    "        transforms.Resize((128, 128)), # resize images\n",
    "        transforms.RandomHorizontalFlip(), # randomly flip image\n",
    "      # transforms.RandomRotation(20), # randomly rotate image 20 degrees\n",
    "        transforms.ColorJitter(brightness = 0.1, contrast = 0.1, saturation = 0.1), # change color of images\n",
    "        transforms.ToTensor(), # turn into 3xHxW tensors\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), # normalize RGB\n",
    "                             (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # create PyTorch datasets\n",
    "    train_dataset = FlowerDataset(train_df, transform = img_transforms)\n",
    "    test_dataset = FlowerDataset(test_df, transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "    # number of flower classes\n",
    "    num_classes = train_df[\"label_int\"].nunique()\n",
    "    print(\"Number of flower classes:\", num_classes)\n",
    "\n",
    "    # device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # create model\n",
    "    model = CNNModel(num_classes).to(device)\n",
    "\n",
    "    # loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4, weight_decay = 1e-4)\n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "\n",
    "    # training loop\n",
    "    epochs = 15 # increase epochs\n",
    "    print(\"Starting training loop..\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/total:.4f}, Acc: {correct/total:.4f}\")\n",
    "\n",
    "    # evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Test Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), \"flower_cnn.pth\")\n",
    "    print(\"Saved model as flower_cnn.pth\")\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    train_flower_cnn()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
